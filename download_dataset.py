#!/usr/bin/env python3
"""
LongEmotion æ•°æ®é›†ä¸‹è½½å’ŒéªŒè¯è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python download_dataset.py --output_dir ./hf_dataset

åŠŸèƒ½:
    1. ä» HuggingFace ä¸‹è½½ LongEmotion æ•°æ®é›†
    2. éªŒè¯æ•°æ®å®Œæ•´æ€§
    3. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
"""

import argparse
import json
import os
from pathlib import Path
from typing import Dict, List, Any

try:
    from huggingface_hub import snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("è­¦å‘Š: huggingface_hub æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install huggingface_hub")


class LongEmotionDownloader:
    """LongEmotion æ•°æ®é›†ä¸‹è½½å™¨"""
    
    REPO_ID = "LongEmotion/LongEmotion"
    
    EXPECTED_FILES = {
        "Emotion Classification/Emotion_Classification_Emobench.jsonl": 200,
        "Emotion Classification/Emotion_Classification_Finentity.jsonl": 200,
        "Emotion Detection/Emotion_Detection.jsonl": 136,
        "Emotion QA/Emotion_QA.jsonl": 120,
        "Emotion Conversation/Emotion_Conversations.jsonl": 100,
        "Emotion Summary/Emotion_Summary.jsonl": 150,
        "Emotion Summary/Emotion_Summary_origin.jsonl": 150,
        "Emotion Expression/Emotion_Expression_Situations.json": None,  # JSONæ–‡ä»¶
        "Emotion Expression/Emotion_Expression_Questionnaires.json": None,  # JSONæ–‡ä»¶
    }
    
    def __init__(self, output_dir: str = "./hf_dataset"):
        self.output_dir = Path(output_dir)
        self.stats = {}
        
    def download(self) -> bool:
        """ä¸‹è½½æ•°æ®é›†"""
        if not HF_AVAILABLE:
            print("é”™è¯¯: éœ€è¦å®‰è£… huggingface_hub")
            print("è¿è¡Œ: pip install huggingface_hub")
            return False
        
        print(f"æ­£åœ¨ä» HuggingFace ä¸‹è½½ {self.REPO_ID} ...")
        print(f"ä¿å­˜åˆ°: {self.output_dir.absolute()}")
        
        try:
            local_dir = snapshot_download(
                repo_id=self.REPO_ID,
                repo_type='dataset',
                local_dir=str(self.output_dir),
            )
            print(f"âœ“ ä¸‹è½½æˆåŠŸï¼æ•°æ®ä¿å­˜åœ¨: {local_dir}")
            return True
        except Exception as e:
            print(f"âœ— ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def validate(self) -> bool:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        print("\n" + "="*80)
        print("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        print("="*80)
        
        all_valid = True
        
        for file_path, expected_count in self.EXPECTED_FILES.items():
            full_path = self.output_dir / file_path
            
            if not full_path.exists():
                print(f"âœ— æ–‡ä»¶ç¼ºå¤±: {file_path}")
                all_valid = False
                continue
            
            # è¯»å–æ–‡ä»¶å¹¶ç»Ÿè®¡
            try:
                if file_path.endswith('.jsonl'):
                    with open(full_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        actual_count = len(lines)
                        
                    # éªŒè¯æ¯è¡Œéƒ½æ˜¯æœ‰æ•ˆçš„JSON
                    for i, line in enumerate(lines[:5], 1):  # æ£€æŸ¥å‰5è¡Œ
                        try:
                            json.loads(line)
                        except json.JSONDecodeError as e:
                            print(f"âœ— {file_path} ç¬¬{i}è¡ŒJSONæ ¼å¼é”™è¯¯: {e}")
                            all_valid = False
                    
                    if expected_count and actual_count != expected_count:
                        print(f"âš  {file_path}: é¢„æœŸ{expected_count}æ¡ï¼Œå®é™…{actual_count}æ¡")
                    else:
                        print(f"âœ“ {file_path}: {actual_count}æ¡æ•°æ®")
                    
                    self.stats[file_path] = actual_count
                    
                else:  # JSONæ–‡ä»¶
                    with open(full_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    if isinstance(data, list):
                        count = len(data)
                    elif isinstance(data, dict):
                        count = f"{len(data)} å­—æ®µ"
                    else:
                        count = "1 å¯¹è±¡"
                    
                    print(f"âœ“ {file_path}: {count}")
                    self.stats[file_path] = str(count)
                    
            except Exception as e:
                print(f"âœ— {file_path} è¯»å–å¤±è´¥: {e}")
                all_valid = False
        
        print("="*80)
        if all_valid:
            print("âœ“ æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡ï¼")
        else:
            print("âš  éƒ¨åˆ†æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
        
        return all_valid
    
    def analyze(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
        print("="*80)
        
        # Emotion Classification - Emobench
        self._analyze_emotion_classification_emobench()
        
        # Emotion Classification - Finentity
        self._analyze_emotion_classification_finentity()
        
        # Emotion Detection
        self._analyze_emotion_detection()
        
        # Emotion QA
        self._analyze_emotion_qa()
        
        # Emotion Conversation
        self._analyze_emotion_conversation()
        
        # Emotion Summary
        self._analyze_emotion_summary()
        
        print("="*80)
    
    def _load_jsonl(self, file_path: str) -> List[Dict]:
        """åŠ è½½JSONLæ–‡ä»¶"""
        full_path = self.output_dir / file_path
        with open(full_path, 'r', encoding='utf-8') as f:
            return [json.loads(line) for line in f]
    
    def _load_json(self, file_path: str) -> Any:
        """åŠ è½½JSONæ–‡ä»¶"""
        full_path = self.output_dir / file_path
        with open(full_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _analyze_emotion_classification_emobench(self):
        """åˆ†æ Emotion Classification Emobench æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion Classification/Emotion_Classification_Emobench.jsonl")
            avg_length = sum(d.get('length', 0) for d in data) / len(data)
            emotions = set(d['label'] for d in data)
            
            print(f"\nğŸ“Š Emotion Classification (Emobench)")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            print(f"   å¹³å‡é•¿åº¦: {avg_length:.2f} tokens")
            print(f"   æƒ…ç»ªç±»åˆ«æ•°: {len(emotions)}")
            print(f"   æƒ…ç»ªç¤ºä¾‹: {', '.join(list(emotions)[:5])}, ...")
        except Exception as e:
            print(f"âœ— åˆ†æ Emobench å¤±è´¥: {e}")
    
    def _analyze_emotion_classification_finentity(self):
        """åˆ†æ Emotion Classification Finentity æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion Classification/Emotion_Classification_Finentity.jsonl")
            avg_length = sum(d.get('token_length', 0) for d in data) / len(data)
            emotions = set(d['label'] for d in data)
            
            print(f"\nğŸ“Š Emotion Classification (Finentity)")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            print(f"   å¹³å‡é•¿åº¦: {avg_length:.2f} tokens")
            print(f"   æƒ…ç»ªç±»åˆ«: {', '.join(sorted(emotions))}")
        except Exception as e:
            print(f"âœ— åˆ†æ Finentity å¤±è´¥: {e}")
    
    def _analyze_emotion_detection(self):
        """åˆ†æ Emotion Detection æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion Detection/Emotion_Detection.jsonl")
            avg_length = sum(d.get('length', 0) for d in data) / len(data)
            
            print(f"\nğŸ“Š Emotion Detection")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            print(f"   å¹³å‡é•¿åº¦: {avg_length:.2f} tokens")
        except Exception as e:
            print(f"âœ— åˆ†æ Emotion Detection å¤±è´¥: {e}")
    
    def _analyze_emotion_qa(self):
        """åˆ†æ Emotion QA æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion QA/Emotion_QA.jsonl")
            sources = set(d['source'] for d in data)
            
            print(f"\nğŸ“Š Emotion QA")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            print(f"   æ¥æºæ–‡çŒ®æ•°: {len(sources)}")
        except Exception as e:
            print(f"âœ— åˆ†æ Emotion QA å¤±è´¥: {e}")
    
    def _analyze_emotion_conversation(self):
        """åˆ†æ Emotion Conversation æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion Conversation/Emotion_Conversations.jsonl")
            total_stages = sum(len(d.get('stages', [])) for d in data)
            
            print(f"\nğŸ“Š Emotion Conversation")
            print(f"   å¯¹è¯æ•°: {len(data)}")
            print(f"   æ€»è½®æ¬¡: {total_stages}")
            print(f"   å¹³å‡è½®æ¬¡: {total_stages / len(data):.2f}")
        except Exception as e:
            print(f"âœ— åˆ†æ Emotion Conversation å¤±è´¥: {e}")
    
    def _analyze_emotion_summary(self):
        """åˆ†æ Emotion Summary æ•°æ®"""
        try:
            data = self._load_jsonl("Emotion Summary/Emotion_Summary.jsonl")
            
            print(f"\nğŸ“Š Emotion Summary")
            print(f"   æ ·æœ¬æ•°: {len(data)}")
            print(f"   å­—æ®µ: causes, symptoms, treatment_process, treatment_effect")
        except Exception as e:
            print(f"âœ— åˆ†æ Emotion Summary å¤±è´¥: {e}")
    
    def generate_report(self, output_file: str = "dataset_report.txt"):
        """ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š"""
        report_path = self.output_dir / output_file
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("LongEmotion æ•°æ®é›†æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            f.write("æ–‡ä»¶ç»Ÿè®¡:\n")
            f.write("-"*80 + "\n")
            for file_path, count in self.stats.items():
                f.write(f"{file_path}: {count}\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write(f"æŠ¥å‘Šç”Ÿæˆäº: {self.output_dir.absolute()}\n")
            f.write("="*80 + "\n")
        
        print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    parser = argparse.ArgumentParser(
        description="LongEmotion æ•°æ®é›†ä¸‹è½½å’ŒéªŒè¯å·¥å…·"
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./hf_dataset',
        help='æ•°æ®é›†ä¿å­˜ç›®å½• (é»˜è®¤: ./hf_dataset)'
    )
    parser.add_argument(
        '--skip_download',
        action='store_true',
        help='è·³è¿‡ä¸‹è½½ï¼Œä»…éªŒè¯å·²æœ‰æ•°æ®'
    )
    parser.add_argument(
        '--skip_analysis',
        action='store_true',
        help='è·³è¿‡è¯¦ç»†åˆ†æ'
    )
    
    args = parser.parse_args()
    
    downloader = LongEmotionDownloader(args.output_dir)
    
    # ä¸‹è½½
    if not args.skip_download:
        if not downloader.download():
            return
    
    # éªŒè¯
    if not downloader.validate():
        print("\nâš  æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
        return
    
    # åˆ†æ
    if not args.skip_analysis:
        downloader.analyze()
    
    # ç”ŸæˆæŠ¥å‘Š
    downloader.generate_report()
    
    print("\n" + "="*80)
    print("âœ“ å…¨éƒ¨å®Œæˆï¼")
    print(f"æ•°æ®ä½ç½®: {Path(args.output_dir).absolute()}")
    print("="*80)


if __name__ == "__main__":
    main()
